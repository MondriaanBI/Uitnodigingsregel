{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5346835-6776-4d82-a7e7-26cea6b63a17",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb2f282f-6e86-4de3-a67d-2ba4e2451dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to .py when done\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import datetime\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3752762-2c26-4d0f-8966-a6c0c89bd4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAMM01\\OneDrive - ROC Mondriaan\\Documenten\\Workspace\\CEDA\\GitHub\\Uitnodigingsregel\\Uitnodigingsregel\n"
     ]
    }
   ],
   "source": [
    "# Construct the path to move up two directory levels\n",
    "higher_dir = os.path.abspath('../../')\n",
    "os.chdir(higher_dir)\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "from module.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6237c09b-1589-422b-b090-797a528d89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "user_data_train_path = 'data/raw/user_data/train.csv'\n",
    "user_data_pred_path = 'data/raw/user_data/pred.csv'\n",
    "synth_train_path = 'data/raw/synth_data_train.csv'\n",
    "synth_pred_path = 'data/raw/synth_data_pred.csv'\n",
    "\n",
    "\n",
    "# Check if train.csv and pred.csv exist in user_data folder\n",
    "if os.path.exists(user_data_train_path) and os.path.exists(user_data_pred_path):\n",
    "    train_df = pd.read_csv(user_data_train_path, sep = '\\t')\n",
    "    pred_df = pd.read_csv(user_data_pred_path, sep = '\\t')\n",
    "else:\n",
    "    train_df = pd.read_csv(synth_train_path, sep = '\\t')\n",
    "    pred_df = pd.read_csv(synth_pred_path, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b1d1ea9-5c5d-4213-979b-eb246eddf43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add verbose for progress bar \n",
    "# Random Forest Regressor model\n",
    "def randomforestregressormodel_train(dataset_train, random_state = RANDOM_SEED):\n",
    "    X = dataset_train.drop(\"Dropout\", axis=1).values\n",
    "    y = dataset_train.Dropout.values\n",
    "\n",
    "  \n",
    "    rf = RandomForestRegressor(random_state=random_state)\n",
    "    parameters = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'max_features': [3, 4, 5],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [2, 3, 5],\n",
    "    'n_estimators': [100, 200, 300]}\n",
    "\n",
    "    grid_model = GridSearchCV(rf, parameters, n_jobs = -1)\n",
    "    grid_model.fit(X, y) \n",
    "    best_params = grid_model.best_params_\n",
    "    best_rf_model = RandomForestRegressor(**best_params)\n",
    "    best_rf_model.fit(X, y) \n",
    "    return best_rf_model\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0248424-d8f8-48c8-a4e5-125e84c3eba2",
   "metadata": {},
   "source": [
    "best_rf_model = randomforestregressormodel_train (train_df)\n",
    "\n",
    "# Save model \n",
    "joblib.dump(best_rf_model, 'models/random_forest_regressor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe5b2413-f0a6-4198-a242-4c89b676c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add LASSO and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40cad2c2-b9cd-4513-ab70-ea6b72a22d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "## Use MinMax scaler to ensure all values are between 0 and 1 and prevent model bias. Alternatives are RobustScaler (based on percentiles so \n",
    "## immune to large outliers) or StandardScaler (if the data has a normal distribution) \n",
    "\n",
    "train_df_scaled = MinMaxScaler().fit_transform(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fb2ea45-57dc-49c5-8d40-6bd8c3862eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een functie aan voor de conventionele evaluatie van de voorspelling\n",
    "def conventionele_evaluatie(model=\"lasso\", data = train_df_scaled):\n",
    "    # Bepaal de evaluatie categorie\n",
    "    data[\"True Positive {}\".format(model)] = np.where(((data[\"yhat2\"] >= 0.5) & (data[\"y_test\"] == 1)), 1, 0)\n",
    "    data[\"False Positive {}\".format(model)] = np.where(((data[\"yhat2\"] >= 0.5) & (data[\"y_test\"] == 0)), 1, 0)\n",
    "    data[\"True Negative {}\".format(model)] = np.where(((data[\"yhat2\"] <= 0.5) & (data[\"y_test\"] == 0)), 1, 0)\n",
    "    data[\"False Negative {}\".format(model)] = np.where(((data[\"yhat2\"] <= 0.5) & (data[\"y_test\"] == 1)), 1, 0)\n",
    "\n",
    "    conv_precision = data[\"True Positive {}\".format(model)].sum()/ (data[\"True Positive {}\".format(model)].sum() + data[\"False Positive {}\".format(model)].sum())\n",
    "    conv_sensitivity = data[\"True Positive {}\".format(model)].sum()/ (data[\"True Positive {}\".format(model)].sum() + data[\"False Negative {}\".format(model)].sum())\n",
    "    # Maak een simpele confusionmatrix\n",
    "    total_dict = {\"Actual 0\": [data[\"True Negative {}\".format(model)].sum(), data[\"False Positive {}\".format(model)].sum()],\n",
    "                  \"Actual 1\": [data[\"False Negative {}\".format(model)].sum(), data[\"True Positive {}\".format(model)].sum()]}\n",
    "    total = pd.DataFrame(data=total_dict, index = [\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "    return total, conv_precision, conv_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e6d64-3869-4443-88a2-514ad24bde56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso model after changes \n",
    "def lassomodel(train_df_scaled):\n",
    "    X = train_df_scaled.drop(\"Dropout\", axis=1).values\n",
    "    y = train_df_scaled.Dropout.values\n",
    "    ## Use LassoCV (cross-validation) to find the best alpha for LASSO model\n",
    "    lasso_cv_model = LassoCV(cv=5, random_state=RANDOM_SEED)\n",
    "    lasso_cv_model.fit(X_train, y_train)\n",
    "\n",
    "  \n",
    "    return yhat2_data, lasso_coefficients\n",
    "\n",
    "## Run model\n",
    "yhat2_data_lasso, lasso_coefficients = lassomodel() \n",
    "#yhat2_data_lasso = dynamische_evaluatie(model=\"lasso\", data = yhat2_data_lasso)\n",
    "total_lasso, conv_precision_lasso, conv_sensitivity_lasso = conventionele_evaluatie(model=\"lasso\", data = yhat2_data_lasso)\n",
    "yhat2_data_lasso['precisionlasso'] = yhat2_data_lasso['precisionlasso'].apply(lambda x: x*100)\n",
    "yhat2_data_lasso['recalllasso'] = yhat2_data_lasso['recalllasso'].apply(lambda x: x*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e21325-fb0b-4acb-a2ef-09f615189114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54343834-341f-4305-8b0d-99c4838bc6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59f548-f1b0-4e91-9bdc-f4f55324ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "  ## Predict \n",
    "    yhat = model.predict(X_train)   \n",
    "    yhat2 = model.predict(X_test)\n",
    "\n",
    "\n",
    "    ## Sort results\n",
    "    yhat2_data[\"yhat2_rank\"] = yhat2_data[\"yhat2\"].rank(method = 'dense', ascending=False)\n",
    "    yhat2_data = yhat2_data.sort_values(by=[\"yhat2\"], ascending=False).reset_index(drop=True)\n",
    "    yhat2_data[\"i\"] = yhat2_data.index + 1\n",
    "\n",
    "    ## Add column with binary output of yhat2 based on mean \n",
    "    mean_lasso = yhat2_data.loc[:, 'yhat2'].mean()\n",
    "    df_total = pd.DataFrame(data = yhat2_data['yhat2'])\n",
    "    df_total.rename(columns={'yhat2': 'binary_output'}, inplace=True)\n",
    "    df_total1 = df_total.applymap(lambda x: 1 if x > mean_lasso else 0)\n",
    "    yhat2_data = pd.concat([yhat2_data, df_total1], axis=1)\n",
    "    \n",
    "    ## Features to column names \n",
    "    feature_names_after_preprocessing = dataset.drop(\"Dropout\", axis=1).columns.tolist()  # Mismatch after use of SMOTE because of increased dimensions of X_train\n",
    "    lasso_coefficients = get_coefficient_table(X_train, model)  # extra\n",
    "    lasso_coefficients.index = feature_names_after_preprocessing# extra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cbdb0-2874-405b-af18-d49469877c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e4782-d6bd-468d-a8d8-2a100d3aaa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4ab5f-9940-4c0c-9134-244024162b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
