---
title: "Inzichten Modelresultaten"
format: 
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    theme: cosmo
    code-fold: true
    output-dir: reports
    lang: nl
    toc-title: "Inhoudsopgave"
    css: styles.css
    highlight-style: github
    fig-width: 10
    fig-height: 6
    fig-align: center
    fig-format: png
    fig-dpi: 300
    html-math-method: katex
    self-contained: true
execute:
  echo: false
  warning: false
  fig-format: png
  fig-dpi: 300
---

```{python}
#| label: setup
#| echo: false

import sys
from module.plot import (
    generate_precision_plot, 
    generate_sensitivity_plot, 
    generate_svm_importance_plot,
    generate_stoplight_evaluation,
    save_model_metrics,
    save_threshold_analysis,
    extract_model_data,
    sort_and_filter_data,
    process_evaluation_results,
    display_model_results,
    get_coefficient_table,
    get_top_svm_features,
    analyze_missing_data,
    parse_model_metrics,
    display_top_features
)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
import warnings
from sklearn.model_selection import train_test_split
from sklearn import tree
from contextlib import redirect_stdout, redirect_stderr
warnings.filterwarnings('ignore')

# Import custom modules
from module.dataset import *
from module.modeling.train import *
from module.modeling.predict import *
from module.features import *
import yaml

# Load config and apply settings
def load_settings(config_file, settings_type='default'):
    with open(config_file, 'r') as file:
        config = yaml.safe_load(file)
    if settings_type == 'default':
        settings = config['default_settings']
    elif settings_type == 'custom':
        settings = config['custom_settings']
    else:
        raise ValueError("No settings found. Choose 'default' or 'custom'.")
    return settings

config_file = 'config.yaml'
settings = load_settings(config_file, 'default')
globals().update(settings)

# Set plot styling
sns.set_theme(style="whitegrid")
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['axes.formatter.useoffset'] = False
plt.rcParams['axes.formatter.limits'] = (-2, 2)
sns.set_style("whitegrid")
sns.set_context("notebook", font_scale=1.2)

# Function to detect the correct separator for CSV files
def detect_separator(file_path, target_column='Dropout'):
    """
    Automatically detect the separator used in a CSV file.
    
    Parameters:
        file_path: Path to the CSV file
        target_column: Name of the target column to look for
        
    Returns:
        The detected separator (comma, tab, semicolon, etc.)
    """
    separators = [',', '\t', ';', '|']
    
    for sep in separators:
        try:
            # Read first few lines to test
            df_sample = pd.read_csv(file_path, sep=sep, nrows=5, engine='python')
            
            # Check if parsing was successful and contains target column
            if len(df_sample.columns) > 1 and target_column in df_sample.columns:
                return sep
        except Exception as e:
            continue
    
    # Fallback to comma if nothing works
    return ','

# Data loading: use user data if present, otherwise use synthetic data (training data only)
if os.path.exists(user_data_dir_train):
    # Detect separator for user data
    train_sep = detect_separator(user_data_dir_train, dropout_column)
    train_df = pd.read_csv(user_data_dir_train, sep=train_sep, engine='python')
    data_type = "instelling"
else:
    # Detect separator for synthetic data
    train_sep = detect_separator(synth_data_dir_train, dropout_column)
    train_df = pd.read_csv(synth_data_dir_train, sep=train_sep, engine='python')
    data_type = "synthetische"

train_cleaned = basic_cleaning(train_df)
# Create a copy of train_cleaned for prediction data (needed for processing functions)
pred_cleaned = train_cleaned.copy()
train_cleaned, pred_cleaned = remove_single_value_columns(train_cleaned, pred_cleaned)
train_processed, pred_processed = convert_categorical_to_dummies(train_cleaned, pred_cleaned, dropout_column, separator)
train_df_sdd, pred_df_sdd = standardize_dataset(train_processed, pred_processed, dropout_column, separator)

# Message is shown based on data_type
print(f"Opmerking: Deze analyse is uitgevoerd op {data_type} data.")
```

# Model Aanbeveling en Evaluatie

```{python}
#| label: stoplight-eval
#| output: asis
#| echo: false
#| warning: false

try:
    # --- 1. Train/test split ---
    # Use processed data for RF, scaled for Lasso/SVM
    X_train, X_test, y_train, y_test = train_test_split(
        train_processed.drop(dropout_column, axis=1),
        train_processed[dropout_column],
        test_size=0.2,
        random_state=42
    )
    X_train_sdd, X_test_sdd, y_train_sdd, y_test_sdd = train_test_split(
        train_df_sdd.drop(dropout_column, axis=1),
        train_df_sdd[dropout_column],
        test_size=0.2,
        random_state=42
    )
    # Prepare test sets for evaluation
    test_data_rf = pd.concat([X_test, y_test], axis=1)
    test_data_sdd = pd.concat([X_test_sdd, y_test_sdd], axis=1)

    # Make test data available globally for plotting functions
    globals()['test_data_rf'] = test_data_rf
    globals()['test_data_sdd'] = test_data_sdd

    # --- 2. Train or load models ---
    if retrain_models:
        # Suppress GridSearchCV output during training
        with open(os.devnull, 'w') as fnull:
            with redirect_stdout(fnull), redirect_stderr(fnull):
                best_rf_model = randomforestregressormodel_train(pd.concat([X_train, y_train], axis=1), random_seed, dropout_column, rf_parameters)
                best_lasso_model = lassoregressionmodel_train(pd.concat([X_train_sdd, y_train_sdd], axis=1), random_seed, dropout_column, alpha_range)
                best_svm_model = supportvectormachinemodel_train(pd.concat([X_train_sdd, y_train_sdd], axis=1), random_seed, dropout_column, svm_parameters)
    else:
        best_rf_model = joblib.load('models/random_forest_regressor.joblib')
        best_lasso_model = joblib.load('models/lasso_regression.joblib')
        best_svm_model = joblib.load('models/support_vector_machine.joblib')

    # --- 3. Modular stoplight evaluation on test set ---
    model_predictions = {
        'Random Forest': (test_data_rf, best_rf_model, False),
        'Lasso': (test_data_sdd, best_lasso_model, True),
        'SVM': (test_data_sdd, best_svm_model, True)
    }
    
    evaluation_results = generate_stoplight_evaluation(
        model_predictions,
        invite_pct=20
    )

    # --- 4. Save metrics and threshold analysis on test set ---
    with warnings.catch_warnings():
        warnings.simplefilter('ignore')
        save_model_metrics(
            train_data=pd.concat([X_train, y_train], axis=1),
            train_data_scaled=pd.concat([X_train_sdd, y_train_sdd], axis=1),
            validation_data=test_data_rf,
            validation_data_scaled=test_data_sdd,
            rf_model=best_rf_model,
            lasso_model=best_lasso_model,
            svm_model=best_svm_model
        )
        save_threshold_analysis(
            train_data=pd.concat([X_train, y_train], axis=1),
            train_data_scaled=pd.concat([X_train_sdd, y_train_sdd], axis=1),
            validation_data=test_data_rf,
            validation_data_scaled=test_data_sdd,
            rf_model=best_rf_model,
            lasso_model=best_lasso_model,
            svm_model=best_svm_model
        )
except Exception as e:
    print(f"Error in stoplight evaluation: {e}")
    import traceback
    traceback.print_exc()

# Process evaluation results using imported functions
model_results, best_model, best_metrics, recommendation_display, recommendation_text = process_evaluation_results(evaluation_results)

print(recommendation_display)
print("\n" + recommendation_text + " Het best presterende model is " + 
      best_model + 
      ", dat de beste balans laat zien tussen precisie en recall bij een selectie van 20% van de studenten voor uitnodiging. " +
      "Dit impliceert dat dit model het meest effectief is in het identificeren van studenten met een verhoogd risico op uitval.")

print("\n<div class='disclaimer' style='background-color: #fff3cd; padding: 15px; margin: 20px 0; border-left: 4px solid #ffc107;'>")
print("<strong>Belangrijke opmerking:</strong>")
print("<p>Deze evaluatie is automatisch gegenereerd en dient als richtlijn. De uiteindelijke beslissing over het gebruik van het model ligt bij de gebruiker. Het is belangrijk om:</p>")
print("<ul>")
print("  <li>De resultaten kritisch te evalueren in de context van uw specifieke situatie</li>")
print("  <li>De beperkingen van het model te begrijpen</li>")
print("  <li>De ethische implicaties van het gebruik te overwegen</li>")
print("  <li>De resultaten te valideren met domeinexperts</li>")
print("</ul>")
print("<p>De gebruiker is zelf verantwoordelijk voor de interpretatie en het gebruik van de modelresultaten.</p>")
print("</div>")

print("\nVoor meer informatie over de gebruikte modellen wordt verwezen naar Hoofdstuk 2. " +
      "Verdere details over de precisie, recall en model-specifieke analyses zijn te vinden in de latere hoofdstukken van dit rapport.")

print("\n**Prestaties van het aanbevolen model:**")
print(f"- Precisie: {best_metrics['precision']:.1f}%")
print(f"- Recall: {best_metrics['recall']:.1f}%")
print(f"- Status: {best_metrics['status']}")
print(f"\n**Samenvatting:**")
print(best_metrics['dutch_summary'])
print("\n---\n")

print("\n**Toelichting:**")
print("- **Precisie (%):** Percentage van de uitgenodigde studenten dat daadwerkelijk uitvalt")
print("- **Recall (%):** Percentage van alle uitvallers dat wordt ge√Ødentificeerd")
print("- **% Uitgenodigd:** Percentage van alle studenten dat wordt uitgenodigd")
```

## Wat te controleren bij een slechte aanbeveling

Wanneer een model een slechte aanbeveling doet, zijn er verschillende onderdelen in dit rapport die u kunt raadplegen om mogelijke oorzaken te achterhalen. Begin bij 3.3 Model Metrics om te controleren op tekenen van overfitting of underfitting, zoals grote verschillen tussen training en testresultaten of een negatieve R¬≤.

Bekijk vervolgens hoofdstuk 4 (Data Kwaliteit) om te zien of er sprake is van ontbrekende of onbetrouwbare data die het model kan hebben be√Ønvloed.

Tot slot bieden hoofdstukken 5, 6 en 7 inzicht in de prestaties en relevantie van individuele features. Een lage bijdrage of onverwacht gedrag van belangrijke variabelen kan verklaren waarom het model geen goede voorspelling doet.

# Gedetailleerde Modellen

## Random Forest

```{python}
#| label: rf-results
#| output: asis
#| echo: false
# Display the stored results for Random Forest
print(display_model_results(model_results, 'Random Forest'))
```

## Lasso

```{python}
#| label: lasso-results
#| output: asis
#| echo: false
# Display the stored results for Lasso
print(display_model_results(model_results, 'Lasso'))
```

## SVM

```{python}
#| label: svm-results
#| output: asis
#| echo: false
# Display the stored results for SVM
print(display_model_results(model_results, 'SVM'))
```

# Model Evaluatie

## Precisie Plot

```{python}
#| label: precision-plot
#| echo: false
#| fig-cap: "Precisie per uitnodigingspercentage voor alle modellen (test set)"
#| fig-width: 10
#| fig-height: 6

# Generate the plot using test set data for consistency with evaluation
# Use test_data_rf for RF, test_data_sdd for Lasso and SVM
fig = generate_precision_plot(test_data_rf, best_rf_model, best_lasso_model, best_svm_model, do_save=False)
plt.tight_layout(); plt.show()
```

## Recall Plot

```{python}
#| label: recall-plot
#| echo: false
#| fig-cap: "Recall per uitnodigingspercentage voor alle modellen (test set)"
#| fig-width: 10
#| fig-height: 6

# Generate the plot using test set data for consistency with evaluation
# Use test_data_rf for RF, test_data_sdd for Lasso and SVM
fig = generate_sensitivity_plot(test_data_rf, best_rf_model, best_lasso_model, best_svm_model, do_save=False)
plt.tight_layout(); plt.show()
```

## Model Metrics

```{python}
#| label: model-metrics
#| output: asis

# Parse model metrics using helper function
metrics_data = parse_model_metrics()

# Print the formatted table
if metrics_data:
    print("\n| Model | R¬≤ (train) | MSE (train) | R¬≤ (test) | MSE (test) |")
    print("|:------|:-----------|:------------|:----------|:-----------|")
    for row in metrics_data:
        print(f"| {row['Model']} | {row['R¬≤ (train)']:>9.3f} | {row['MSE (train)']:>10.3f} | {row['R¬≤ (test)']:>8.3f} | {row['MSE (test)']:>9.3f} |")
else:
    print("\n**Model Metrics Table**")
    print("Geen model metrics data beschikbaar.")

print("""
### Interpretatie van de Metrics

1. **Overfitting:**
   Wanneer een model aanzienlijk beter presteert op de trainingsdata dan op de testdata, kan dit wijzen op overfitting. 
   Het model heeft dan niet alleen de onderliggende patronen geleerd, maar ook de ruis of uitzonderingen in de trainingsset, 
   waardoor het minder goed generaliseert naar nieuwe data.

2. **Underfitting:**
   Als een model zowel op de trainings- als testdata slecht presteert, kan er sprake zijn van underfitting. 
   Dit houdt in dat het model te simpel is om de relevante patronen in de data te herkennen, of dat de gebruikte 
   features onvoldoende informatief zijn.

3. **Negatieve R¬≤-waarde:**
   Een negatieve R¬≤-waarde betekent dat het model slechter presteert dan simpelweg het gemiddelde van de doelvariabele 
   voorspellen. Dit kan wijzen op een verkeerd gekozen model, slecht afgestemde hyperparameters of structurele 
   problemen in de data.
""")
```

# Data Kwaliteit

## Algemene Statistieken
```{python}
#| label: missing-analysis 1
#| output: asis

# Analyze missing data using helper function (using raw data for transparency)
missing_summary, total_missing, total_rows, total_cols, missing_cols = analyze_missing_data(train_df)

# Print summary
print(f"- Totaal aantal rijen: {total_rows:,}")
print(f"- Totaal aantal kolommen: {total_cols}")
print(f"- Totaal aantal missing values: {total_missing:,}")
print(f"- Aantal kolommen met missing values: {missing_cols}")
print(f"- Percentage rijen met minstens √©√©n missing value: {(train_df.isnull().any(axis=1).sum() / total_rows * 100):.1f}%\n")

```

## Missing Values per Kolom
```{python}
#| label: missing-analysis 2
#| output: asis

# Print missing values per column
if total_missing > 0:
    print("\n## Missing Values per Kolom\n")
    print("| Kolom | Type | Aantal Missing | Percentage Missing |")
    print("|:------|:-----|:--------------:|:------------------:|")
    for idx, row in missing_summary[missing_summary['Aantal_Missing'] > 0].sort_values('Aantal_Missing', ascending=False).iterrows():
        print(f"| {idx} | {row['Type']} | {row['Aantal_Missing']:,} | {row['Percentage_Missing']:.1f}% |")
else:
    print("\nGeen missing values gevonden in de dataset.\n")
```

## Heatmap

```{python}
#| label: missing-plots
#| echo: false
#| fig-cap: "Heatmap van missing values in de dataset"
#| fig-height: 8

# Create a heatmap of missing values (using raw data for transparency)
fig, ax = plt.subplots(figsize=(12, 8))
sns.heatmap(train_df.isnull(), yticklabels=False, cbar=False, cmap='viridis', ax=ax)
```

# Lasso Features
## Top 5 Features met Grootste Impact
```{python}
#| label: lasso-features
#| output: asis

# Display top 5 Lasso features using helper function
print(display_top_features(best_lasso_model, train_df_sdd, 'lasso', n_features=5, dropout_column=dropout_column))
```

# Random Forest Features
## Top 5 Features met Grootste Impact
```{python}
#| label: rf-features
#| output: asis

# Display top 5 Random Forest features using helper function
print(display_top_features(best_rf_model, train_processed, 'rf', n_features=5, dropout_column=dropout_column))
```

## Voorbeeld van een beslissingsboom
```{python}
#| label: rf-tree-plot
#| echo: false
#| fig-cap: "Voorbeeld van een beslissingsboom uit het Random Forest model"
#| fig-width: 10
#| fig-height: 6

# Get the first tree from the forest
first_tree = best_rf_model.estimators_[0]

# Create figure
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the tree
tree.plot_tree(
    first_tree,
    feature_names=train_processed.drop(dropout_column, axis=1).columns,
    class_names=['Geen Uitval', 'Uitval'],
    max_depth=3,  # Limit depth for better visibility
    fontsize=8,
    filled=True,
    rounded=True,
    ax=ax
)

plt.tight_layout()
```

# SVM Features
## Top 5 Features met Grootste Impact
```{python}
#| label: svm-features-text
#| output: asis

# Display top 5 SVM features using helper function
print(display_top_features(best_svm_model, train_processed, 'svm', n_features=5, dropout_column=dropout_column))
```

## SVM Feature Importance

```{python}
#| label: svm-importance
#| echo: false
#| fig-cap: "Feature importance voor het SVM model"
#| fig-width: 14
#| fig-height: 10

# Generate the plot
fig = generate_svm_importance_plot(train_processed, best_svm_model, train_data_sdd=train_df_sdd, do_save=False)
```

# Uitleg

```{python}
#| label: all-explanations
#| output: asis

print("""
### Model Uitleg

#### Random Forest Model
Het Random Forest model gebruikt een ensemble van beslissingsbomen om voorspellingen te maken. Dit model is bijzonder goed in het vangen van complexe, niet-lineaire relaties in de data. Door het gebruik van meerdere beslissingsbomen kan het model zowel lineaire als niet-lineaire relaties identificeren. Dit model kan ook complexe interacties tussen features modelleren, zoals de combinatie van studievoortgang en aanwezigheid.

#### Lasso Model
Het Lasso model is een lineair model met L1 regularisatie, wat helpt bij feature selectie. Dit model is transparanter in zijn besluitvorming en kan helpen bij het identificeren van de belangrijkste voorspellende factoren. Door de L1 regularisatie worden minder belangrijke features naar nul gedreven, waardoor we een duidelijk beeld krijgen van welke factoren het meest bijdragen aan de voorspellingen.

#### Support Vector Machine (SVM) Model
Het SVM model zoekt de optimale scheidingslijn tussen klassen met maximale marge. Dit model is vooral effectief in situaties met duidelijke scheiding tussen klassen. Het is bijzonder gevoelig voor de schaal van de features en kan goed omgaan met niet-lineaire relaties tussen features. De feature importance plot toont welke factoren het meest bijdragen aan deze scheiding.

### Stoplight Evaluatie Systeem Uitleg

Het stoplichtsysteem evalueert de bruikbaarheid van voorspellende modellen voor het identificeren van studenten met een verhoogd risico op uitval. Deze evaluatie is gebaseerd op de balans tussen precisie en recall bij verschillende selectiepercentages.

In dit rapport richten we ons specifiek op een scenario waarbij 20% van alle studenten wordt uitgenodigd voor een gesprek. De keuze voor dit percentage is gebaseerd op de praktijk: bij beperkte middelen moeten vooral die studenten worden ge√Ødentificeerd die het hoogste risico op uitval lopen. Het model moet dus effectief zijn in het correct selecteren van deze top 20%.

Hoewel we op basis van dit criterium modellen beoordelen, benadrukken we dat het aanbevolen wordt om uiteindelijk alle studenten te spreken. De modelanalyse dient daarbij als hulpmiddel om prioriteiten te stellen, niet om studenten uit te sluiten. 

Het systeem gebruikt drie kleuren om de bruikbaarheid aan te duiden:

- üü¢ **Groen**: Model presteert goed voor gerichte interventies
  - Precisie ‚â• 40% EN Recall ‚â• 40%
  - Betrouwbaar voor het identificeren van risicostudenten
  
- üü° **Geel**: Model geeft matig signaal
  - Precisie ‚â• 30% EN Recall ‚â• 30%
  - Gebruik met voorzichtigheid, aanvullende informatie aanbevolen
  
- üî¥ **Rood**: Model heeft verbetering nodig
  - Precisie < 30% OF Recall < 30%
  - Niet betrouwbaar voor directe interventies

#### Precisie en Recall Uitleg
- **Precisie:** Het percentage van de uitgenodigde studenten dat daadwerkelijk uitvalt. Een hoge precisie betekent dat we effici√´nt zijn in het identificeren van studenten die echt risico lopen.
- **Recall:** Het percentage van alle uitvallers dat we kunnen identificeren. Een hoge recall betekent dat we de meeste uitvallers kunnen opsporen.
- **Trade-off:** Er is altijd een afweging tussen precisie en recall. Een hoger uitnodigingspercentage leidt tot een hogere recall maar lagere precisie, en vice versa.
""")
```